#!/usr/bin/env python3
"""
æ•°æ®åº“å¤‡ä»½è„šæœ¬
"""
import os
import sys
import subprocess
import gzip
import shutil
from datetime import datetime, timedelta
from pathlib import Path
import argparse
import yaml
import json
from typing import Dict, List, Optional
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from backend.app.core.config import settings


class DatabaseBackup:
    """æ•°æ®åº“å¤‡ä»½ç®¡ç†å™¨"""
    
    def __init__(self, backup_dir: str = "backups"):
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # è§£ææ•°æ®åº“URL
        self.db_config = self._parse_database_url(settings.DATABASE_URL)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
    
    def _setup_logging(self):
        """è®¾ç½®å¤‡ä»½æ—¥å¿—"""
        log_file = self.backup_dir / "backup.log"
        logger.add(
            str(log_file),
            rotation="10 MB",
            retention="30 days",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
        )
    
    def _parse_database_url(self, database_url: str) -> Dict[str, str]:
        """è§£ææ•°æ®åº“è¿æ¥URL"""
        # postgresql://user:password@host:port/database
        try:
            from urllib.parse import urlparse
            parsed = urlparse(database_url)
            
            return {
                "host": parsed.hostname or "localhost",
                "port": str(parsed.port or 5432),
                "database": parsed.path.lstrip('/'),
                "username": parsed.username,
                "password": parsed.password,
                "scheme": parsed.scheme
            }
        except Exception as e:
            logger.error(f"è§£ææ•°æ®åº“URLå¤±è´¥: {e}")
            raise
    
    def create_backup(self, backup_name: Optional[str] = None, 
                     compress: bool = True) -> str:
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = backup_name or f"uma_audit_backup_{timestamp}"
        
        backup_file = self.backup_dir / f"{backup_name}.sql"
        
        try:
            logger.info(f"å¼€å§‹åˆ›å»ºæ•°æ®åº“å¤‡ä»½: {backup_name}")
            
            # æ„å»ºpg_dumpå‘½ä»¤
            cmd = [
                "pg_dump",
                "-h", self.db_config["host"],
                "-p", self.db_config["port"],
                "-U", self.db_config["username"],
                "-d", self.db_config["database"],
                "--verbose",
                "--clean",
                "--create",
                "--if-exists",
                "-f", str(backup_file)
            ]
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            if self.db_config["password"]:
                env["PGPASSWORD"] = self.db_config["password"]
            
            # æ‰§è¡Œå¤‡ä»½å‘½ä»¤
            result = subprocess.run(
                cmd,
                env=env,
                capture_output=True,
                text=True,
                timeout=3600  # 1å°æ—¶è¶…æ—¶
            )
            
            if result.returncode != 0:
                raise Exception(f"å¤‡ä»½å¤±è´¥: {result.stderr}")
            
            # å‹ç¼©å¤‡ä»½æ–‡ä»¶
            if compress:
                compressed_file = f"{backup_file}.gz"
                with open(backup_file, 'rb') as f_in:
                    with gzip.open(compressed_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                
                # åˆ é™¤åŸå§‹æ–‡ä»¶
                backup_file.unlink()
                backup_file = Path(compressed_file)
            
            # è®°å½•å¤‡ä»½ä¿¡æ¯
            backup_info = {
                "name": backup_name,
                "file": str(backup_file),
                "timestamp": timestamp,
                "size": backup_file.stat().st_size,
                "compressed": compress,
                "database": self.db_config["database"],
                "host": self.db_config["host"]
            }
            
            info_file = self.backup_dir / f"{backup_name}.json"
            with open(info_file, 'w') as f:
                json.dump(backup_info, f, indent=2)
            
            logger.success(f"å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_file}")
            logger.info(f"å¤‡ä»½å¤§å°: {backup_file.stat().st_size / 1024 / 1024:.2f} MB")
            
            return str(backup_file)
            
        except subprocess.TimeoutExpired:
            logger.error("å¤‡ä»½è¶…æ—¶")
            raise
        except Exception as e:
            logger.error(f"åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
            # æ¸…ç†å¤±è´¥çš„å¤‡ä»½æ–‡ä»¶
            if backup_file.exists():
                backup_file.unlink()
            raise
    
    def restore_backup(self, backup_file: str, target_database: Optional[str] = None):
        """æ¢å¤æ•°æ®åº“å¤‡ä»½"""
        backup_path = Path(backup_file)
        
        if not backup_path.exists():
            raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
        
        target_db = target_database or self.db_config["database"]
        
        try:
            logger.info(f"å¼€å§‹æ¢å¤æ•°æ®åº“å¤‡ä»½: {backup_file} -> {target_db}")
            
            # å¦‚æœæ˜¯å‹ç¼©æ–‡ä»¶ï¼Œå…ˆè§£å‹
            temp_file = None
            if backup_path.suffix == '.gz':
                temp_file = backup_path.with_suffix('')
                with gzip.open(backup_path, 'rb') as f_in:
                    with open(temp_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                restore_file = temp_file
            else:
                restore_file = backup_path
            
            # æ„å»ºpsqlå‘½ä»¤
            cmd = [
                "psql",
                "-h", self.db_config["host"],
                "-p", self.db_config["port"],
                "-U", self.db_config["username"],
                "-d", target_db,
                "-f", str(restore_file)
            ]
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            if self.db_config["password"]:
                env["PGPASSWORD"] = self.db_config["password"]
            
            # æ‰§è¡Œæ¢å¤å‘½ä»¤
            result = subprocess.run(
                cmd,
                env=env,
                capture_output=True,
                text=True,
                timeout=3600  # 1å°æ—¶è¶…æ—¶
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file and temp_file.exists():
                temp_file.unlink()
            
            if result.returncode != 0:
                raise Exception(f"æ¢å¤å¤±è´¥: {result.stderr}")
            
            logger.success(f"æ•°æ®åº“æ¢å¤æˆåŠŸ: {target_db}")
            
        except subprocess.TimeoutExpired:
            logger.error("æ¢å¤è¶…æ—¶")
            raise
        except Exception as e:
            logger.error(f"æ¢å¤å¤‡ä»½å¤±è´¥: {e}")
            raise
    
    def list_backups(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backups = []
        
        for info_file in self.backup_dir.glob("*.json"):
            try:
                with open(info_file) as f:
                    backup_info = json.load(f)
                    
                # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                backup_file = Path(backup_info["file"])
                if backup_file.exists():
                    backup_info["exists"] = True
                    backup_info["size"] = backup_file.stat().st_size
                else:
                    backup_info["exists"] = False
                
                backups.append(backup_info)
                
            except Exception as e:
                logger.warning(f"è¯»å–å¤‡ä»½ä¿¡æ¯å¤±è´¥ {info_file}: {e}")
        
        return sorted(backups, key=lambda x: x["timestamp"], reverse=True)
    
    def cleanup_old_backups(self, keep_days: int = 30, keep_count: int = 10):
        """æ¸…ç†è¿‡æœŸå¤‡ä»½"""
        backups = self.list_backups()
        cutoff_date = datetime.now() - timedelta(days=keep_days)
        
        # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€è¿‘çš„å¤‡ä»½
        sorted_backups = sorted(backups, key=lambda x: x["timestamp"], reverse=True)
        
        deleted_count = 0
        for i, backup in enumerate(sorted_backups):
            backup_time = datetime.strptime(backup["timestamp"], "%Y%m%d_%H%M%S")
            
            # ä¿ç•™æœ€è¿‘çš„å‡ ä¸ªå¤‡ä»½ï¼Œæˆ–è€…åœ¨æ—¶é—´èŒƒå›´å†…çš„å¤‡ä»½
            if i < keep_count and backup_time >= cutoff_date:
                continue
            
            # åˆ é™¤è¿‡æœŸå¤‡ä»½
            try:
                backup_file = Path(backup["file"])
                info_file = self.backup_dir / f"{backup['name']}.json"
                
                if backup_file.exists():
                    backup_file.unlink()
                if info_file.exists():
                    info_file.unlink()
                
                deleted_count += 1
                logger.info(f"å·²åˆ é™¤è¿‡æœŸå¤‡ä»½: {backup['name']}")
                
            except Exception as e:
                logger.error(f"åˆ é™¤å¤‡ä»½å¤±è´¥ {backup['name']}: {e}")
        
        logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªè¿‡æœŸå¤‡ä»½")
        return deleted_count
    
    def get_backup_statistics(self) -> Dict:
        """è·å–å¤‡ä»½ç»Ÿè®¡ä¿¡æ¯"""
        backups = self.list_backups()
        
        total_size = sum(backup.get("size", 0) for backup in backups if backup.get("exists", False))
        
        stats = {
            "total_backups": len(backups),
            "total_size": total_size,
            "total_size_mb": round(total_size / 1024 / 1024, 2),
            "oldest_backup": None,
            "newest_backup": None,
            "avg_size_mb": 0
        }
        
        if backups:
            stats["oldest_backup"] = min(backups, key=lambda x: x["timestamp"])["timestamp"]
            stats["newest_backup"] = max(backups, key=lambda x: x["timestamp"])["timestamp"]
            
            if stats["total_backups"] > 0:
                stats["avg_size_mb"] = round(stats["total_size_mb"] / stats["total_backups"], 2)
        
        return stats


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®åº“å¤‡ä»½ç®¡ç†")
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # åˆ›å»ºå¤‡ä»½
    create_parser = subparsers.add_parser("create", help="åˆ›å»ºå¤‡ä»½")
    create_parser.add_argument("--name", help="å¤‡ä»½åç§°")
    create_parser.add_argument("--no-compress", action="store_true", help="ä¸å‹ç¼©å¤‡ä»½")
    
    # æ¢å¤å¤‡ä»½
    restore_parser = subparsers.add_parser("restore", help="æ¢å¤å¤‡ä»½")
    restore_parser.add_argument("backup_file", help="å¤‡ä»½æ–‡ä»¶è·¯å¾„")
    restore_parser.add_argument("--target-db", help="ç›®æ ‡æ•°æ®åº“åç§°")
    
    # åˆ—å‡ºå¤‡ä»½
    subparsers.add_parser("list", help="åˆ—å‡ºæ‰€æœ‰å¤‡ä»½")
    
    # æ¸…ç†å¤‡ä»½
    cleanup_parser = subparsers.add_parser("cleanup", help="æ¸…ç†è¿‡æœŸå¤‡ä»½")
    cleanup_parser.add_argument("--keep-days", type=int, default=30, help="ä¿ç•™å¤©æ•°")
    cleanup_parser.add_argument("--keep-count", type=int, default=10, help="ä¿ç•™æ•°é‡")
    
    # ç»Ÿè®¡ä¿¡æ¯
    subparsers.add_parser("stats", help="æ˜¾ç¤ºå¤‡ä»½ç»Ÿè®¡ä¿¡æ¯")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    backup_manager = DatabaseBackup()
    
    try:
        if args.command == "create":
            backup_file = backup_manager.create_backup(
                backup_name=args.name,
                compress=not args.no_compress
            )
            print(f"å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_file}")
            
        elif args.command == "restore":
            backup_manager.restore_backup(
                args.backup_file,
                target_database=args.target_db
            )
            print("æ•°æ®åº“æ¢å¤æˆåŠŸ")
            
        elif args.command == "list":
            backups = backup_manager.list_backups()
            print(f"\nå…±æ‰¾åˆ° {len(backups)} ä¸ªå¤‡ä»½:\n")
            
            for backup in backups:
                status = "âœ…" if backup.get("exists", False) else "âŒ"
                size_mb = backup.get("size", 0) / 1024 / 1024
                print(f"{status} {backup['name']}")
                print(f"   æ—¶é—´: {backup['timestamp']}")
                print(f"   å¤§å°: {size_mb:.2f} MB")
                print(f"   æ–‡ä»¶: {backup['file']}")
                print()
                
        elif args.command == "cleanup":
            deleted = backup_manager.cleanup_old_backups(
                keep_days=args.keep_days,
                keep_count=args.keep_count
            )
            print(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted} ä¸ªè¿‡æœŸå¤‡ä»½")
            
        elif args.command == "stats":
            stats = backup_manager.get_backup_statistics()
            print(f"\nğŸ“Š å¤‡ä»½ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   æ€»å¤‡ä»½æ•°: {stats['total_backups']}")
            print(f"   æ€»å¤§å°: {stats['total_size_mb']} MB")
            print(f"   å¹³å‡å¤§å°: {stats['avg_size_mb']} MB")
            print(f"   æœ€æ—©å¤‡ä»½: {stats['oldest_backup']}")
            print(f"   æœ€æ–°å¤‡ä»½: {stats['newest_backup']}")
            
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()