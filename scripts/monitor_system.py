#!/usr/bin/env python3
"""
ç³»ç»Ÿç›‘æ§è„šæœ¬
"""
import asyncio
import psutil
import time
import json
import aiohttp
import asyncpg
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import argparse
from dataclasses import dataclass, asdict
from loguru import logger
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from backend.app.core.config import settings


@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    memory_available_mb: float
    disk_percent: float
    disk_used_gb: float
    disk_free_gb: float
    network_sent_mb: float
    network_recv_mb: float
    process_count: int
    load_average: List[float]


@dataclass
class DatabaseMetrics:
    """æ•°æ®åº“æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    active_connections: int
    max_connections: int
    database_size_mb: float
    table_count: int
    slow_queries: int
    cache_hit_ratio: float


@dataclass
class ApplicationMetrics:
    """åº”ç”¨æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    response_time_ms: float
    status_code: int
    is_healthy: bool
    error_rate: float
    active_users: int


class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""
    
    def __init__(self, output_dir: str = "monitoring"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.metrics_file = self.output_dir / "system_metrics.jsonl"
        self.alerts_file = self.output_dir / "alerts.log"
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # ç›‘æ§é˜ˆå€¼
        self.thresholds = {
            "cpu_percent": 80.0,
            "memory_percent": 85.0,
            "disk_percent": 90.0,
            "response_time_ms": 5000.0,
            "error_rate": 10.0
        }
    
    def _setup_logging(self):
        """è®¾ç½®ç›‘æ§æ—¥å¿—"""
        log_file = self.output_dir / "monitor.log"
        logger.add(
            str(log_file),
            rotation="50 MB",
            retention="30 days",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
        )
    
    def collect_system_metrics(self) -> SystemMetrics:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            # CPUæŒ‡æ ‡
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # å†…å­˜æŒ‡æ ‡
            memory = psutil.virtual_memory()
            
            # ç£ç›˜æŒ‡æ ‡
            disk = psutil.disk_usage('/')
            
            # ç½‘ç»œæŒ‡æ ‡
            network = psutil.net_io_counters()
            
            # è¿›ç¨‹æ•°é‡
            process_count = len(psutil.pids())
            
            # è´Ÿè½½å¹³å‡å€¼
            load_avg = psutil.getloadavg() if hasattr(psutil, 'getloadavg') else [0, 0, 0]
            
            return SystemMetrics(
                timestamp=datetime.now().isoformat(),
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                memory_used_mb=memory.used / 1024 / 1024,
                memory_available_mb=memory.available / 1024 / 1024,
                disk_percent=disk.percent,
                disk_used_gb=disk.used / 1024 / 1024 / 1024,
                disk_free_gb=disk.free / 1024 / 1024 / 1024,
                network_sent_mb=network.bytes_sent / 1024 / 1024,
                network_recv_mb=network.bytes_recv / 1024 / 1024,
                process_count=process_count,
                load_average=list(load_avg)
            )
            
        except Exception as e:
            logger.error(f"æ”¶é›†ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
            raise
    
    async def collect_database_metrics(self) -> Optional[DatabaseMetrics]:
        """æ”¶é›†æ•°æ®åº“æŒ‡æ ‡"""
        try:
            # è§£ææ•°æ®åº“è¿æ¥URL
            import urllib.parse
            parsed = urllib.parse.urlparse(settings.DATABASE_URL.replace("postgresql://", "postgres://"))
            
            # è¿æ¥æ•°æ®åº“
            conn = await asyncpg.connect(
                host=parsed.hostname,
                port=parsed.port or 5432,
                database=parsed.path.lstrip('/'),
                user=parsed.username,
                password=parsed.password
            )
            
            try:
                # è·å–è¿æ¥æ•°
                active_connections = await conn.fetchval(
                    "SELECT count(*) FROM pg_stat_activity WHERE state = 'active'"
                )
                
                max_connections = await conn.fetchval("SHOW max_connections")
                max_connections = int(max_connections)
                
                # è·å–æ•°æ®åº“å¤§å°
                database_size = await conn.fetchval(
                    "SELECT pg_database_size(current_database())"
                )
                database_size_mb = database_size / 1024 / 1024
                
                # è·å–è¡¨æ•°é‡
                table_count = await conn.fetchval(
                    "SELECT count(*) FROM information_schema.tables WHERE table_schema = 'public'"
                )
                
                # è·å–æ…¢æŸ¥è¯¢æ•°é‡ï¼ˆå‡è®¾æ…¢æŸ¥è¯¢å®šä¹‰ä¸ºæ‰§è¡Œæ—¶é—´>1ç§’ï¼‰
                slow_queries = await conn.fetchval(
                    """
                    SELECT count(*) FROM pg_stat_statements 
                    WHERE mean_time > 1000
                    """
                ) or 0
                
                # è·å–ç¼“å­˜å‘½ä¸­ç‡
                cache_stats = await conn.fetchrow(
                    """
                    SELECT 
                        sum(heap_blks_hit) as hits,
                        sum(heap_blks_hit + heap_blks_read) as total
                    FROM pg_statio_user_tables
                    """
                )
                
                if cache_stats and cache_stats['total'] > 0:
                    cache_hit_ratio = (cache_stats['hits'] / cache_stats['total']) * 100
                else:
                    cache_hit_ratio = 0.0
                
                return DatabaseMetrics(
                    timestamp=datetime.now().isoformat(),
                    active_connections=active_connections,
                    max_connections=max_connections,
                    database_size_mb=database_size_mb,
                    table_count=table_count,
                    slow_queries=slow_queries,
                    cache_hit_ratio=cache_hit_ratio
                )
                
            finally:
                await conn.close()
                
        except Exception as e:
            logger.error(f"æ”¶é›†æ•°æ®åº“æŒ‡æ ‡å¤±è´¥: {e}")
            return None
    
    async def collect_application_metrics(self) -> Optional[ApplicationMetrics]:
        """æ”¶é›†åº”ç”¨æŒ‡æ ‡"""
        try:
            start_time = time.time()
            
            async with aiohttp.ClientSession() as session:
                # å¥åº·æ£€æŸ¥
                async with session.get("http://localhost:8000/health") as response:
                    response_time_ms = (time.time() - start_time) * 1000
                    is_healthy = response.status == 200
                    
                    # è·å–åº”ç”¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    try:
                        async with session.get("http://localhost:8000/metrics") as metrics_response:
                            if metrics_response.status == 200:
                                metrics_data = await metrics_response.json()
                                system_stats = metrics_data.get("system", {})
                                error_rate = system_stats.get("error_rate_percent", 0.0)
                                active_users = 0  # TODO: å®ç°æ´»è·ƒç”¨æˆ·ç»Ÿè®¡
                            else:
                                error_rate = 0.0
                                active_users = 0
                    except:
                        error_rate = 0.0
                        active_users = 0
                    
                    return ApplicationMetrics(
                        timestamp=datetime.now().isoformat(),
                        response_time_ms=response_time_ms,
                        status_code=response.status,
                        is_healthy=is_healthy,
                        error_rate=error_rate,
                        active_users=active_users
                    )
                    
        except Exception as e:
            logger.error(f"æ”¶é›†åº”ç”¨æŒ‡æ ‡å¤±è´¥: {e}")
            return None
    
    def check_alerts(self, system_metrics: SystemMetrics, 
                    db_metrics: Optional[DatabaseMetrics],
                    app_metrics: Optional[ApplicationMetrics]):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        alerts = []
        
        # ç³»ç»Ÿå‘Šè­¦
        if system_metrics.cpu_percent > self.thresholds["cpu_percent"]:
            alerts.append(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {system_metrics.cpu_percent:.1f}%")
        
        if system_metrics.memory_percent > self.thresholds["memory_percent"]:
            alerts.append(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {system_metrics.memory_percent:.1f}%")
        
        if system_metrics.disk_percent > self.thresholds["disk_percent"]:
            alerts.append(f"ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {system_metrics.disk_percent:.1f}%")
        
        # æ•°æ®åº“å‘Šè­¦
        if db_metrics:
            connection_ratio = db_metrics.active_connections / db_metrics.max_connections
            if connection_ratio > 0.8:
                alerts.append(f"æ•°æ®åº“è¿æ¥æ•°è¿‡é«˜: {db_metrics.active_connections}/{db_metrics.max_connections}")
            
            if db_metrics.cache_hit_ratio < 90:
                alerts.append(f"æ•°æ®åº“ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½: {db_metrics.cache_hit_ratio:.1f}%")
        
        # åº”ç”¨å‘Šè­¦
        if app_metrics:
            if not app_metrics.is_healthy:
                alerts.append(f"åº”ç”¨å¥åº·æ£€æŸ¥å¤±è´¥: HTTP {app_metrics.status_code}")
            
            if app_metrics.response_time_ms > self.thresholds["response_time_ms"]:
                alerts.append(f"åº”ç”¨å“åº”æ—¶é—´è¿‡é•¿: {app_metrics.response_time_ms:.1f}ms")
            
            if app_metrics.error_rate > self.thresholds["error_rate"]:
                alerts.append(f"åº”ç”¨é”™è¯¯ç‡è¿‡é«˜: {app_metrics.error_rate:.1f}%")
        
        # è®°å½•å‘Šè­¦
        if alerts:
            timestamp = datetime.now().isoformat()
            with open(self.alerts_file, 'a') as f:
                for alert in alerts:
                    f.write(f"{timestamp} | ALERT | {alert}\n")
                    logger.warning(f"å‘Šè­¦: {alert}")
    
    def save_metrics(self, system_metrics: SystemMetrics,
                    db_metrics: Optional[DatabaseMetrics],
                    app_metrics: Optional[ApplicationMetrics]):
        """ä¿å­˜æŒ‡æ ‡æ•°æ®"""
        try:
            metrics_data = {
                "system": asdict(system_metrics),
                "database": asdict(db_metrics) if db_metrics else None,
                "application": asdict(app_metrics) if app_metrics else None
            }
            
            with open(self.metrics_file, 'a') as f:
                f.write(json.dumps(metrics_data, ensure_ascii=False) + '\n')
                
        except Exception as e:
            logger.error(f"ä¿å­˜æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
    
    async def run_once(self):
        """è¿è¡Œä¸€æ¬¡ç›‘æ§æ£€æŸ¥"""
        try:
            logger.info("å¼€å§‹æ”¶é›†ç³»ç»ŸæŒ‡æ ‡")
            
            # æ”¶é›†å„ç±»æŒ‡æ ‡
            system_metrics = self.collect_system_metrics()
            db_metrics = await self.collect_database_metrics()
            app_metrics = await self.collect_application_metrics()
            
            # æ£€æŸ¥å‘Šè­¦
            self.check_alerts(system_metrics, db_metrics, app_metrics)
            
            # ä¿å­˜æŒ‡æ ‡
            self.save_metrics(system_metrics, db_metrics, app_metrics)
            
            logger.info("ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å®Œæˆ")
            
            return {
                "system": asdict(system_metrics),
                "database": asdict(db_metrics) if db_metrics else None,
                "application": asdict(app_metrics) if app_metrics else None
            }
            
        except Exception as e:
            logger.error(f"ç›‘æ§æ£€æŸ¥å¤±è´¥: {e}")
            raise
    
    async def run_continuous(self, interval: int = 60):
        """æŒç»­ç›‘æ§"""
        logger.info(f"å¼€å§‹æŒç»­ç›‘æ§ï¼Œé—´éš” {interval} ç§’")
        
        while True:
            try:
                await self.run_once()
                await asyncio.sleep(interval)
                
            except KeyboardInterrupt:
                logger.info("æ¥æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºç›‘æ§")
                break
            except Exception as e:
                logger.error(f"ç›‘æ§å¼‚å¸¸: {e}")
                await asyncio.sleep(interval)
    
    def get_recent_metrics(self, hours: int = 24) -> List[Dict]:
        """è·å–æœ€è¿‘çš„æŒ‡æ ‡æ•°æ®"""
        if not self.metrics_file.exists():
            return []
        
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = []
        
        try:
            with open(self.metrics_file, 'r') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        timestamp_str = data.get("system", {}).get("timestamp")
                        if timestamp_str:
                            timestamp = datetime.fromisoformat(timestamp_str)
                            if timestamp >= cutoff_time:
                                recent_metrics.append(data)
                    except:
                        continue
            
            return sorted(recent_metrics, key=lambda x: x["system"]["timestamp"])
            
        except Exception as e:
            logger.error(f"è¯»å–æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
            return []


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç³»ç»Ÿç›‘æ§")
    parser.add_argument("--interval", type=int, default=60, help="ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--once", action="store_true", help="åªè¿è¡Œä¸€æ¬¡")
    parser.add_argument("--output-dir", default="monitoring", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--recent", type=int, help="æ˜¾ç¤ºæœ€è¿‘Nå°æ—¶çš„æ•°æ®")
    
    args = parser.parse_args()
    
    monitor = SystemMonitor(output_dir=args.output_dir)
    
    try:
        if args.recent:
            # æ˜¾ç¤ºæœ€è¿‘çš„ç›‘æ§æ•°æ®
            metrics = monitor.get_recent_metrics(hours=args.recent)
            print(f"\nğŸ“Š æœ€è¿‘ {args.recent} å°æ—¶çš„ç›‘æ§æ•°æ®:\n")
            
            for data in metrics[-10:]:  # æ˜¾ç¤ºæœ€è¿‘10æ¡
                system = data["system"]
                print(f"æ—¶é—´: {system['timestamp']}")
                print(f"CPU: {system['cpu_percent']:.1f}%")
                print(f"å†…å­˜: {system['memory_percent']:.1f}%")
                print(f"ç£ç›˜: {system['disk_percent']:.1f}%")
                
                if data["application"]:
                    app = data["application"]
                    print(f"åº”ç”¨: å“åº”æ—¶é—´ {app['response_time_ms']:.1f}ms, å¥åº·çŠ¶æ€ {'âœ…' if app['is_healthy'] else 'âŒ'}")
                
                print("-" * 50)
                
        elif args.once:
            # è¿è¡Œä¸€æ¬¡
            result = await monitor.run_once()
            print(json.dumps(result, indent=2, ensure_ascii=False))
            
        else:
            # æŒç»­ç›‘æ§
            await monitor.run_continuous(interval=args.interval)
            
    except KeyboardInterrupt:
        print("\nç›‘æ§å·²åœæ­¢")
    except Exception as e:
        logger.error(f"ç›‘æ§å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())